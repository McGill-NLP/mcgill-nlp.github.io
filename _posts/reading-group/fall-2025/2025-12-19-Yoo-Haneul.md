---
title: "Enhancing Multilingual LLMs with Code-Switching"
venue: KAIST
names: Haneul Yoo
author: Haneul Yoo
tags:
- NLP RG
categories:
    - Reading-Group
    - Fall-2025
layout: archive
classes:
    - wide
    - no-sidebar
---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

The [NLP Reading Group]({% link _pages/reading-group.md %}) is excited to host [Haneul Yoo](https://haneul-yoo.github.io/) who will be giving a talk **in-person** on "Enhancing Multilingual LLMs with Code-Switching"

## Talk Description

Large language models (LLMs) have advanced rapidly, but they are significantly underdeveloped for languages other than English. In this talk, we present three recent studies that leverage code-switching to enhance and evaluate multilingual LLMs. We begin with Code-Switching Red-Teaming (CSRT), a multilingual adversarial evaluation framework that reveals LLMs are significantly more vulnerable to harmful behavior when prompted with code-switched inputs—particularly involving low-resource languages—highlighting an unintended correlation between language resource availability and safety alignment. To address these challenges, the second part introduces Code-Switching Curriculum Learning (CSCL), a training strategy inspired by human language acquisition that progressively fine-tunes LLMs using token-level, sentence-level, and monolingual data. CSCL improves cross-lingual alignment, performance in low-resource languages, and safety robustness, demonstrating that code-switching serves not only as an effective probing lens but also as a practical tool for equitable multilingual adaptation. We finally present Code-Switching In-Context Learning (CSICL), an inference-time technique that facilitates latent translation process of LLMs via code-switching and improves their multilingual reasoning.

## Speaker Bio

Haneul Yoo is a final-year Ph.D. candidate in the School of Computing at KAIST, advised by Professor Alice Oh. She is currently a visiting academic at NYU, hosted by Professor Kyunghyun Cho. Previously, she served as a lecturer at Boostcamp AI Tech, supported by NAVER Connect Foundation. She has also interned at NAVER AI Lab, Upstage, KEPCO Research Institute, and CSIRO.  Her research interests primarily lie in machine learning (ML) and natural language processing (NLP), including but not limited to data-centric, multilingual NLP and NLP applications in education. She has worked on (1) advancing multilingual language modeling, (2) developing resources and evaluation, especially in low‑resource languages, and (3) assisting English as a Foreign Language (EFL) learning through LLM‑driven education innovation.
## Logistics

Date: December 19th<br>
Time: 2:00PM <br>
Location: H04 or via Google Meet (See email)
