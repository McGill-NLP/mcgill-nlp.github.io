---
title: Compositional Generalization in Dependency Parsing
venue: Annual Meeting of the Association for Computational Linguistics
openAccessPdf:
  url: https://aclanthology.org/2022.acl-long.448.pdf
  status: HYBRID
  license: CCBY
  disclaimer: 'Notice: This abstract is extracted from the open access paper or abstract
    available at https://arxiv.org/abs/2110.06843, which is subject to the license
    by the author or copyright owner provided with this content. Please go to the
    source to verify the license and copyright information for your use.'
names: Emily Goodwin, Siva Reddy, T. O’Donnell, Dzmitry Bahdanau
tags:
- Annual Meeting of the Association for Computational Linguistics
link: https://arxiv.org/abs/2110.06843
author: Emily Goodwin
categories: Publications
layout: paper

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Compositionality— the ability to combine familiar units like words into novel phrases and sentences— has been the focus of intense interest in artificial intelligence in recent years. To test compositional generalization in semantic parsing, Keysers et al. (2020) introduced Compositional Freebase Queries (CFQ). This dataset maximizes the similarity between the test and train distributions over primitive units, like words, while maximizing the compound divergence: the dissimilarity between test and train distributions over larger structures, like phrases. Dependency parsing, however, lacks a compositional generalization benchmark. In this work, we introduce a gold-standard set of dependency parses for CFQ, and use this to analyze the behaviour of a state-of-the art dependency parser (Qi et al., 2020) on the CFQ dataset. We find that increasing compound divergence degrades dependency parsing performance, although not as dramatically as semantic parsing performance. Additionally, we find the performance of the dependency parser does not uniformly degrade relative to compound divergence, and the parser performs differently on different splits with the same compound divergence. We explore a number of hypotheses for what causes the non-uniform degradation in dependency parsing performance, and identify a number of syntactic structures that drive the dependency parser’s lower performance on the most challenging splits.