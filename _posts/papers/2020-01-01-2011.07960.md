---
title: Explicitly Modeling Syntax in Language Models with Incremental Parsing and
  a Dynamic Oracle
venue: North American Chapter of the Association for Computational Linguistics
openAccessPdf:
  url: https://aclanthology.org/2021.naacl-main.132.pdf
  status: HYBRID
  license: CCBY
  disclaimer: 'Notice: This abstract is extracted from the open access paper or abstract
    available at https://arxiv.org/abs/2011.07960, which is subject to the license
    by the author or copyright owner provided with this content. Please go to the
    source to verify the license and copyright information for your use.'
names: Yikang Shen, Shawn Tan, Alessandro Sordoni, Siva Reddy, Aaron C. Courville
tags:
- North American Chapter of the Association for Computational Linguistics
link: https://arxiv.org/abs/2011.07960
author: Siva Reddy
categories: Publications
layout: paper

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing, and syntactic generalization tests while using fewer parameters than other models.