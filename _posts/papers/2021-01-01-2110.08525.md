---
title: The Power of Prompt Tuning for Low-Resource Semantic Parsing
venue: Annual Meeting of the Association for Computational Linguistics
openAccessPdf:
  url: https://aclanthology.org/2022.acl-short.17.pdf
  status: HYBRID
  license: CCBY
  disclaimer: 'Notice: This abstract is extracted from the open access paper or abstract
    available at https://arxiv.org/abs/2110.08525, which is subject to the license
    by the author or copyright owner provided with this content. Please go to the
    source to verify the license and copyright information for your use.'
names: Nathan Schucher, Siva Reddy, H. D. Vries
tags:
- Annual Meeting of the Association for Computational Linguistics
link: https://arxiv.org/abs/2110.08525
author: Siva Reddy
categories: Publications
layout: paper

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks. In this paper, we investigate prompt tuning for semantic parsingâ€”the task of mapping natural language utterances onto formal meaning representations. On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines. We also conduct ablation studies across different model scales and target representations, finding that, with increasing model scale, prompt tuned T5 models improve at generating target representations that are far from the pre-training distribution.