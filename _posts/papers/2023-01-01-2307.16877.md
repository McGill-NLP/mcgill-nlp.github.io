---
title: Evaluating Correctness and Faithfulness of Instruction-Following Models for
  Question Answering
venue: Transactions of the Association for Computational Linguistics
openAccessPdf:
  url: https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00667/2374170/tacl_a_00667.pdf
  status: GOLD
  license: CCBY
  disclaimer: 'Notice: This abstract is extracted from the open access paper or abstract
    available at https://arxiv.org/abs/2307.16877, which is subject to the license
    by the author or copyright owner provided with this content. Please go to the
    source to verify the license and copyright information for your use.'
names: Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lù, Nicholas Meade, Siva Reddy
tags:
- Transactions of the Association for Computational Linguistics
link: https://arxiv.org/abs/2307.16877
author: Vaibhav Adlakha
categories: Publications
layout: paper

---

*{{ page.names }}*

**{{ page.venue }}**

{% include display-publication-links.html pub=page %}

## Abstract

Instruction-following models are attractive alternatives to fine-tuned approaches for question answering (QA). By simply prepending relevant documents and an instruction to their input, these models can be adapted to various information domains and tasks without additional training. However, these models tend to produce verbose responses with supplementary information, which makes traditional QA metrics like exact match (EM) and F1 unreliable for accurately quantifying model performance. In this work, we evaluate instruction-following models along two fronts: 1) how well they satisfy user’s information need (correctness), and 2) whether they disseminate information supported by the provided knowledge (faithfulness). Guided by human evaluation and analysis, we highlight the shortcomings of traditional metrics for both correctness and faithfulness and propose simple token-overlap metrics that correlate highly with human judgments. Our analysis reveals that for correctness, instruction-following models perform comparably to models specifically fine-tuned for that task. However, they struggle to accurately judge the relevance of the provided knowledge and often hallucinate in their responses. We hope our work encourages more holistic evaluation of instruction-following models for QA. Our code and human annotation data is available at https://github.com/McGill-NLP/instruct-qa.